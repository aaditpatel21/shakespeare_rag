# **ðŸŽ­ Shakespeare RAG: High-Precision Hybrid Retrieval Engine**

A full-stack, production-grade Retrieval-Augmented Generation (RAG) system capable of answering complex queries about Shakespeare's works with **High Precision** and **High Recall**.

Unlike naive RAG systems, this engine employs a **Two-Stage Hybrid Pipeline** (Dense \+ Sparse Retrieval) followed by a **Cross-Encoder Re-ranker** to solve the "Lost in the Middle" phenomenon and handle specific quote retrieval effectively.

*(Figure 1: Sensitivity Analysis showing the impact of Retrieval Window (k1) and Context Size (k2) on Answer Accuracy)*

## **ðŸš€ Live Demo**

* **Frontend:** https://main.d1np57mpw4ip2u.amplifyapp.com 
* **API Documentation:** https://mtsre2cmst.us-east-1.awsapprunner.com/docs

## **ðŸ§  Key Engineering Features**

### **1\. Hybrid Search Architecture**

Standard vector search often fails on specific phrases (e.g., searching for *"Merry War"* returns chunks about battles, not flirting).

* **Solution:** I implemented a **Hybrid Retrieval** system combining:  
  * **Semantic Search (pgvector):** Captures conceptual queries ("Why is Romeo sad?").  
  * **Keyword Search (tsvector):** Captures exact quotes and entities ("Merry war", "Yorick").  
  * **Multi-Query Expansion:** An LLM pre-processor generates 3 variations of the user's query to maximize recall.

### **2\. Two-Stage Retrieval Funnel**

To balance cost and accuracy, the system uses a funnel approach:

* **Stage 1 (Broad):** Retrieve **50-75 candidates** using efficient HNSW indices.  
* **Stage 2 (Precise):** Re-rank candidates using a **Cross-Encoder** (ms-marco-MiniLM-L-6-v2) to score relevance.  
* **Stage 3 (Focused):** Pass only the **Top 5** chunks to the LLM (Gemini 2.5 Flash) to minimize hallucination.

### **3\. "Lazy" User Persistence**

* Instead of forcing users to login, the system uses **Anonymous Session IDs** generated by the frontend.  
* The backend silently registers users in Postgres upon their first message, allowing for persistent **Chat History** and **Contextual Conversation Memory** without friction.

## **ðŸ“Š Performance Metrics**

| Metric | Naive RAG (Baseline) | Advanced Hybrid RAG (Final) | Lift |
| :---- | :---- | :---- | :---- |
| **Recall@5** | 42.0% | **79.0%** | \+37% |
| **Answer Accuracy** | 47.3% | **82.5%** | \+35.2% |
| **Quote Retrieval** | 43.0% | **88.0%** | \+45% |

*\> Note: The dramatic increase in Quote Retrieval is due to the implementation of BM25/Keyword search alongside Vectors.*

## **ðŸ› ï¸ Tech Stack**

### **Backend & AI**

* **Framework:** FastAPI (Async/Sync Hybrid Architecture)  
* **LLM:** Google Gemini 2.5 Flash  
* **Vector DB:** PostgreSQL (pgvector \+ tsvector) on AWS RDS
* **Embedding Model:** all-MiniLM-L6-v2 (384d)  
* **Re-ranker:** cross-encoder/ms-marco-MiniLM-L-6-v2

### **Frontend**

* **Framework:** React (Vite)  
* **Styling:** Custom CSS (Dark Mode \+ Glassmorphism)  
* **State Management:** LocalStorage \+ React Hooks

### **DevOps**

* **Containerization:** Docker (Multi-stage builds)  
* **Hosting:** AWS App Runner / ECR \+ Amplify (Frontend)  
* **CI/CD:** Manual Docker Push \-\> Automatic Cloud Deploy

## **ðŸ“‚ Architecture**

sequenceDiagram  
    participant User  
    participant React as âš›ï¸ React Frontend  
    participant API as âš¡ FastAPI  
    participant DB as ðŸ˜ Postgres (Vector+Text)  
    participant LLM as ðŸ§  Gemini

    User-\>\>React: "Does he die?"  
    React-\>\>API: POST /chat (Session ID: 123\)  
    API-\>\>DB: Fetch History (User 123\)  
    DB-\>\>API: Returns: "User asked about Romeo..."  
    API-\>\>LLM: Rewrites Query \-\> "Does Romeo die?"  
      
    par Parallel Search  
        API-\>\>DB: Vector Search ("Does Romeo die?")  
        API-\>\>DB: Keyword Search ("Romeo", "Die")  
    end  
      
    DB-\>\>API: Returns 75 Chunks  
    API-\>\>API: Re-rank & Filter to Top 5  
    API-\>\>LLM: Generate Answer with Context  
    LLM-\>\>API: "Yes, by poison..."  
    API-\>\>DB: Save Chat History  
    API-\>\>React: Return JSON {answer, sources}

## **ðŸ”§ Local Installation**

**1\. Clone the repo**

git clone \[https://github.com/aaditpatel21/shakespeare_rag.git](https://github.com/aaditpatel21/shakespeare_rag.git)  
cd shakespeare-rag

2\. Setup Environment  
Create a .env file in the root:  
GEMINI\_API\_KEY=your\_key  
DATABASE\_URL=postgresql://user:pass@localhost:5432/shakespeare

**3\. Run with Docker (Recommended)**

docker build \-t shakespeare-api .  
docker run \-p 8000:8000 \--env-file .env shakespeare-api

**4\. Run Frontend**

cd frontend  
npm install  
npm run dev

## **ðŸ§  Engineering Challenges & Learnings**

* **The "40 Chunk" Ceiling:** I initially struggled with a recall plateau at 58% regardless of k size. Debugging revealed that the default Postgres HNSW index has an ef\_search limit of 40\. Tuning this parameter dynamically in the query session unlocked a 15% boost in recall.  
* **Vector Blindness:** The model consistently failed on specific queries like "Merry War" because semantic search treated it as "Happy Conflict." Implementing a Hybrid Search with tsvector solved this "vocabulary mismatch" problem.  
* **Cost Optimization:** By running re-ranking locally and filtering from 50 \-\> 5 chunks, I reduced the token load sent to Gemini by \~87%, keeping the project well within the free tier limits.